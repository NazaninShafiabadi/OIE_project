{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scoring Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "from preprocessing import parse_txt_file_complex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "-6jvYPOzDD5O"
      },
      "outputs": [],
      "source": [
        "def match_score(g, p):\n",
        "    \"\"\"\n",
        "    Input: a gold tuple g and a predicted tuple p\n",
        "    Output: the match score between g and p\n",
        "\n",
        "    (A side-effect of this scoring method:\n",
        "    If the system has correctly identified a piece of information but has not\n",
        "    put the words in the correct arguments of the extracted tuple, those\n",
        "    misplaced words will not be considered for the score)\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    \n",
        "    # Checking that there is at least (one shared word in each of the first three parts\n",
        "    if all(g[part] and p[part] and (set(g[part]) & set(p[part])) for part in ['arg1', 'rel', 'arg2']):\n",
        "        # Looping through the first three parts in g and p\n",
        "        for part in ['arg1', 'rel', 'arg2']:\n",
        "            if g[part] and p[part]:\n",
        "                # Computing the overlap of the tokens in the part of g and p\n",
        "                overlap = len(set(g[part]).intersection(set(p[part])))\n",
        "                score += overlap\n",
        "\n",
        "    # Normalizing the score by dividing by the total number of tokens in g and p\n",
        "    g_len = sum(len(arg) for arg in list(g.values())[:3] if arg)\n",
        "    p_len = sum(len(arg) for arg in list(p.values())[:3] if arg)\n",
        "    score /= (g_len + p_len)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "def exact_match(G, P):\n",
        "    # Check for exact matches \n",
        "    num_exact_matches = 0\n",
        "    exact_matches = []\n",
        "\n",
        "    if P:\n",
        "        for g in G:\n",
        "            for p in P:\n",
        "                exact_match = True if all(g[part] == p[part] for part in g.keys()) else False\n",
        "                if exact_match:\n",
        "                    exact_matches.append([g, p])\n",
        "                    num_exact_matches += 1\n",
        "    \n",
        "    return num_exact_matches, exact_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "OR_5YLHYDZdH"
      },
      "outputs": [],
      "source": [
        "def find_best_match(g, P):\n",
        "    \"\"\"\n",
        "    Input: a gold tuple g and a list of predicted tuples P\n",
        "    Output: the predicted tuple p_best in P that best matches g\n",
        "    \"\"\"\n",
        "    best_score = 0\n",
        "    p_best = None\n",
        "\n",
        "    # Looping through each predicted tuple in P\n",
        "    if P:\n",
        "        for p in P:\n",
        "            score = match_score(g, p)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                p_best = p\n",
        "\n",
        "    return p_best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "LNZUtov4GXqh"
      },
      "outputs": [],
      "source": [
        "def match(G, P):\n",
        "    \"\"\"\n",
        "    Input: a list of gold tuples G and a list of predicted tuples P\n",
        "    Output: a list of matching pairs M, and the lists of unmatched gold tuples UG and unmatched predicted tuples UP\n",
        "    \"\"\"\n",
        "    M = []\n",
        "    UG = []\n",
        "    UP = []\n",
        "\n",
        "    # Looping through each gold tuple in G\n",
        "    for g in G:\n",
        "        # Finding the tuple in the predicted tuples P that best matches g\n",
        "        p_best = find_best_match(g, P)\n",
        "\n",
        "        if p_best: # and score > threshold:\n",
        "            M.append((g, p_best))\n",
        "            P.remove(p_best)\n",
        "        else:\n",
        "            UG.append(g)\n",
        "\n",
        "    # Adding the remaining tuples in P to UP\n",
        "    UP = P\n",
        "\n",
        "    return M, UG, UP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "Fe2YOTPDAL68"
      },
      "outputs": [],
      "source": [
        "def precision_sys(matching_pairs, unmatched_predictions):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    - matching_pairs: a list of pairs of matching gold and predicted tuples\n",
        "    - unmatched_predictions: a list of prediction tuples not found in the reference\n",
        "\n",
        "    Output: full system precision of predicted and reference tuples at the token level\n",
        "    (precision: the proportion of extracted words that are found in the reference)\n",
        "    \"\"\"\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "\n",
        "    for (gold, pred) in matching_pairs:\n",
        "        pred_args = pred.keys()\n",
        "        # Summing the length of the intersection between matching predicted and gold tuples and adding the value to the precision numerator\n",
        "        numerator += sum(len(set(pred[arg]) & set(gold[arg])) for arg in pred_args if pred[arg] and gold[arg])\n",
        "\n",
        "    if matching_pairs:\n",
        "        _, matched_predictions = zip(*matching_pairs)\n",
        "        all_predictions = list(matched_predictions) + unmatched_predictions\n",
        "    elif unmatched_predictions:\n",
        "        all_predictions = unmatched_predictions\n",
        "    else:\n",
        "        all_predictions = []\n",
        "\n",
        "    for pred in all_predictions:\n",
        "        pred_args = pred.keys()\n",
        "        # Summing the lengths of all prediction tuples\n",
        "        denominator += sum(len(pred[arg]) for arg in pred_args if pred[arg])\n",
        "\n",
        "    try:\n",
        "        precision_sys = numerator / denominator\n",
        "    except ZeroDivisionError:\n",
        "        precision_sys = 0\n",
        "\n",
        "    return precision_sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "Hm7k2GzDDUrX"
      },
      "outputs": [],
      "source": [
        "def recall_sys(matching_pairs, unmatched_references):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    - matching_pairs: a list of pairs of matching gold and predicted tuples\n",
        "    - unmatched_references: a list of reference tuples not found in the predictions\n",
        "\n",
        "    Output: full system recall of predicted and reference tuples at the token level\n",
        "    (recall: the proportion of reference words found in the systemsâ€™ predictions)\n",
        "    \"\"\"\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "\n",
        "    for (gold, pred) in matching_pairs:\n",
        "        gold_args = gold.keys()\n",
        "        # Summing the length of the intersection between matching predicted and gold tuples and adding the value to the recall numerator\n",
        "        numerator += sum(len(set(pred[arg]) & set(gold[arg])) for arg in gold_args if pred[arg] and gold[arg])\n",
        "\n",
        "    if matching_pairs:\n",
        "        matched_references, _ = zip(*matching_pairs)\n",
        "        all_references = list(matched_references) + unmatched_references\n",
        "    else:\n",
        "        all_references = unmatched_references\n",
        "\n",
        "    for gold in all_references:\n",
        "        gold_args = gold.keys()\n",
        "        # Summing the lengths of all reference tuples\n",
        "        denominator += sum(len(gold[arg]) for arg in gold_args if gold[arg])\n",
        "\n",
        "    try:\n",
        "        recall_sys = numerator / denominator\n",
        "    except ZeroDivisionError:\n",
        "        recall_sys = 0\n",
        "\n",
        "    return recall_sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "egq2eeiDEavZ"
      },
      "outputs": [],
      "source": [
        "def F1_sys(precision, recall):\n",
        "    try:\n",
        "        F1_sys = (2 * precision * recall) / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        F1_sys = 0\n",
        "\n",
        "    return F1_sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Xf-fZ5BGE3vK"
      },
      "outputs": [],
      "source": [
        "def scorer(ref_data, pred_data):\n",
        "    \"\"\"\n",
        "    Input: a reference dataset G and an OIE system Tsys\n",
        "    Output: the precision, recall, and F1 scores of Tsys on G\n",
        "    \"\"\"\n",
        "    pair_matches = 0\n",
        "    total_num_exact_matches = 0\n",
        "    total_exact_matches = []\n",
        "\n",
        "    for sentence in ref_data:\n",
        "        gold_tuples = ref_data[sentence]\n",
        "        pred_tuples = pred_data.get(sentence, None)\n",
        "\n",
        "        # Move to the next sentence if the system has not predicted anything for this sentence\n",
        "        if not pred_tuples:\n",
        "            precision = 0\n",
        "            recall = 0\n",
        "            F1 = 0\n",
        "\n",
        "        # Finding the matching pairs and the unmatched tuples using the matching function\n",
        "        matching_pairs, unmatched_gold, unmatched_pred = match(gold_tuples, pred_tuples)\n",
        "        pair_matches += len(matching_pairs)\n",
        "\n",
        "        num_exact_matches, exact_matching_pairs = exact_match(gold_tuples, pred_tuples)\n",
        "        total_num_exact_matches += num_exact_matches\n",
        "        total_exact_matches.extend(exact_matching_pairs)\n",
        "\n",
        "        precision = precision_sys(matching_pairs, unmatched_pred)\n",
        "        recall = recall_sys(matching_pairs, unmatched_gold)\n",
        "        F1 = F1_sys(precision, recall)\n",
        "\n",
        "    return precision, recall, F1, pair_matches, total_num_exact_matches, total_exact_matches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the pipeline on Stanford Open IE and ReVerb tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "references = parse_txt_file_complex('IE_AKGC_GOLD.txt')\n",
        "reverb_predictions = parse_txt_file_complex('results/Reverb_annotations.txt')\n",
        "stanford_predictions = parse_txt_file_complex('results/stanford-openie-output.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ReVerb:\n",
            "Precision: 0.5789473684210527, Recall: 0.34375, F1: 0.4313725490196078, Matches: 51, Exact Matches: 7\n"
          ]
        }
      ],
      "source": [
        "reverb_precision, reverb_recall, reverb_f1, reverb_matches, reverb_exact_matches, reverb_total_exact_matches = scorer(references, reverb_predictions)\n",
        "print('ReVerb:')\n",
        "print(f'Precision: {reverb_precision}, Recall: {reverb_recall}, F1: {reverb_f1}, Matches: {reverb_matches}, Exact Matches: {reverb_exact_matches}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stanford:\n",
            "Precision: 0.328125, Recall: 0.328125, F1: 0.328125, Matches: 60, Exact Matches: 0\n"
          ]
        }
      ],
      "source": [
        "stanford_precision, stanford_recall, stanford_f1, stanford_matches, stanford_exact_matches, stanford_total_exact_matches = scorer(references, stanford_predictions)\n",
        "print('Stanford:')\n",
        "print(f'Precision: {stanford_precision}, Recall: {stanford_recall}, F1: {stanford_f1}, Matches: {stanford_matches}, Exact Matches: {stanford_exact_matches}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
